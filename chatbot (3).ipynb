{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQXjMtGfuRDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c758ed-972f-4748-de4e-456094df8c47"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gfMwLw9v0-P",
        "outputId": "025975a1-b26d-4c21-aad6-48862f0edcef"
      },
      "source": [
        "!pip install schedule"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: schedule in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGnxUjHvwKON",
        "outputId": "3c0c4b18-a0a4-4189-98bd-6a82d9361a11"
      },
      "source": [
        "!pip install trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: trax in /usr/local/lib/python3.7/dist-packages (1.3.9)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (from trax) (0.2.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from trax) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from trax) (4.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from trax) (2.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from trax) (3.2.2)\n",
            "Requirement already satisfied: t5 in /usr/local/lib/python3.7/dist-packages (from trax) (0.9.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from trax) (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from trax) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from trax) (1.19.5)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from trax) (0.1.66+cuda110)\n",
            "Requirement already satisfied: funcsigs in /usr/local/lib/python3.7/dist-packages (from trax) (1.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from trax) (5.4.8)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.1.6)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (21.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (5.1.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->trax) (2.5.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->trax) (1.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.8.1+cu101)\n",
            "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.1.19)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5->trax) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.0.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.1.95)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.1.5)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.7/dist-packages (from t5->trax) (4.3.0.dev202106060107)\n",
            "Requirement already satisfied: seqio in /usr/local/lib/python3.7/dist-packages (from t5->trax) (0.0.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: transformers>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from t5->trax) (4.6.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from t5->trax) (1.5.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->trax) (1.12)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (57.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.53.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.12.5)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.34.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.36.2)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5->trax) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5->trax) (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5->trax) (0.10.3)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5->trax) (2.0.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.30.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text->trax) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G43LBMttvR5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa62ae0-7a1c-4df7-f4a9-44934f21a8bf"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "!pip list | grep trax"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.9                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAh8FNDlvp9r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bf05bca-8b10-4a09-ad35-77679f3a713c"
      },
      "source": [
        "DATA_FILE = \"data.json\"\n",
        "DATA_FILE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1FWb7-bxoFY"
      },
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/Multilingual_ChatBot/Dataset\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbUQxTUq0T0T"
      },
      "source": [
        "DIALOGUE_DB = {}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tLTMGWp0XHk"
      },
      "source": [
        "VOCAB_FILE = \"en_32k.subword\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XMCFRBtZ_v8"
      },
      "source": [
        "VOCAB_FILE = \"en_32k.subword\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRKaDV2UZApO"
      },
      "source": [
        "VOCAB_DIR = \"/content/drive/My Drive/Multilingual_ChatBot/coursera-natural-language-processing-specialization/4 - Natural Language Processing with Attention Models/Week 4/data/vocabs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzGxCfWaMor"
      },
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "\n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD9Z1TmUaRhR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a976eae-cc37-497a-a725-9437d65877da"
      },
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of dialogues is: 10438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BApbpdgOcgba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feefd70-23e8-4a35-a1ab-c9612b2c7dad"
      },
      "source": [
        "print(list(DIALOGUE_DB.keys())[0:7])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SNG01856.json', 'SNG0129.json', 'PMUL1635.json', 'MUL2168.json', 'SNG0073.json', 'SNG01445.json', 'MUL2105.json']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eZ_73TbcjrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9554ced5-ecd1-491f-fedd-0817418bae72"
      },
      "source": [
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['goal', 'log'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZhipwVccsqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79a4b27-017b-4000-e56d-5a53e03fe84a"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attraction': {},\n",
              " 'hospital': {},\n",
              " 'hotel': {},\n",
              " 'message': [\"You want to book a <span class='emphasis'>taxi</span>. The taxi should go to <span class='emphasis'>pizza hut fen ditton</span> and should depart from <span class='emphasis'>saint john's college</span>\",\n",
              "  \"The taxi should <span class='emphasis'>leave after 17:15</span>\",\n",
              "  \"Make sure you get <span class='emphasis'>car type</span> and <span class='emphasis'>contact number</span>\"],\n",
              " 'police': {},\n",
              " 'restaurant': {},\n",
              " 'taxi': {'fail_info': {},\n",
              "  'info': {'departure': \"saint john's college\",\n",
              "   'destination': 'pizza hut fen ditton',\n",
              "   'leaveAt': '17:15'},\n",
              "  'reqt': ['car type', 'phone']},\n",
              " 'train': {}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmaqS4kVcwBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4f6cc2-4ce8-418e-f15b-b31a146a93a9"
      },
      "source": [
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metadata': {},\n",
              " 'text': \"I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaoA1YR_c23p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5c213f-907c-480c-e865-3a3261ae6d6d"
      },
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1:  I would like a taxi from Saint John's college to Pizza Hut Fen Ditton.\n",
            " Person 2:  What time do you want to leave and what time do you want to arrive by?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGkGJNjbc7Dp"
      },
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''    \n",
        "    len_msg_log = len(data_db[file]['log'])    \n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    for i in range(len_msg_log):            \n",
        "        cur_log = data_db[file]['log'][i]['text']        \n",
        "        if i % 2 == 0:                   \n",
        "            result += delimiter_1\n",
        "        else: \n",
        "            result += delimiter_2        \n",
        "        result += cur_log\n",
        "    return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i96C0MgodEAi"
      },
      "source": [
        "import numpy as np\n",
        "import trax\n",
        "def test_get_conversation(target):\n",
        "\n",
        "    data = {'file1.json': {'log':[{'text': 'hi'},\n",
        "                                  {'text': 'hello'},\n",
        "                                  {'text': 'nice'}]},\n",
        "            'file2.json':{'log':[{'text': 'a b'}, \n",
        "                                 {'text': ''}, \n",
        "                                 {'text': 'good '}, \n",
        "                                 {'text': 'no?'}]}}    \n",
        "    res1 = target('file1.json', data)\n",
        "    res2 = target('file2.json', data)\n",
        "    \n",
        "    expected1 = ' Person 1: hi Person 2: hello Person 1: nice'\n",
        "    expected2 = ' Person 1: a b Person 2:  Person 1: good  Person 2: no?'\n",
        "\n",
        "    success = 0\n",
        "    fails = 0    \n",
        "    try:\n",
        "        assert res1 == expected1\n",
        "        success += 1\n",
        "    except ValueError:\n",
        "        print('Error in test 1 \\nResult  : ', res1, 'x \\nExpected: ', expected1)\n",
        "        fails += 1\n",
        "    try:\n",
        "        assert res2 == expected2\n",
        "        success += 1\n",
        "    except:\n",
        "        print('Error in test 2 \\nResult  : ', res2, ' \\nExpected: ', expected2)\n",
        "        fails += 1\n",
        "            \n",
        "    if fails == 0:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print('\\033[92m', success,\" Tests passed\")\n",
        "        print('\\033[91m', fails, \" Tests failed\")\n",
        "def test_reversible_layer_forward(target):\n",
        "    f1 = lambda x: x + 2\n",
        "    g1 = lambda x: x * 3    \n",
        "    f2 = lambda x: x + 1\n",
        "    g2 = lambda x: x * 2    \n",
        "    input_vector1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "    expected1 = np.array([8, 10, 12, 14, 29, 36, 43, 50])\n",
        "    \n",
        "    input_vector2 = np.array([1] * 128)\n",
        "    expected2 = np.array([3] * 64 + [7] * 64)\n",
        "    \n",
        "    success = 0\n",
        "    fails = 0\n",
        "    try:\n",
        "        res = target(input_vector1, f1, g1)\n",
        "        assert isinstance(res, np.ndarray)\n",
        "        success += 1\n",
        "    except:\n",
        "        print('Wrong type! Output is not of type np.ndarray')\n",
        "        fails += 1\n",
        "    try:\n",
        "        res = target(input_vector1, f1, g1)\n",
        "        assert np.allclose(res, expected1)\n",
        "        success += 1\n",
        "    except ValueError:\n",
        "        print('Error in test 1 \\nResult  : ', res, 'x \\nExpected: ', expected1)\n",
        "        fails += 1\n",
        "    try:\n",
        "        res = target(input_vector2, f2, g2)\n",
        "        assert np.allclose(res, expected2)\n",
        "        success += 1\n",
        "    except:\n",
        "        print('Error in test 2 \\nResult  : ', res, ' \\nExpected: ', expected2)\n",
        "        fails += 1            \n",
        "    if fails == 0:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print('\\033[92m', success,\" Tests passed\")\n",
        "        print('\\033[91m', fails, \" Tests failed\")\n",
        "def test_reversible_layer_reverse(target):    \n",
        "    f1 = lambda x: x + 2\n",
        "    g1 = lambda x: x * 3    \n",
        "    f2 = lambda x: x + 1\n",
        "    g2 = lambda x: x * 2    \n",
        "    input_vector1 = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "    expected1 = np.array([-3,  0,  3,  6,  2,  0, -2, -4])    \n",
        "    input_vector2 = np.array([1] * 128)\n",
        "    expected2 = np.array([1] * 64 + [-1] * 64)    \n",
        "    success = 0\n",
        "    fails = 0\n",
        "    try:\n",
        "        res = target(input_vector1, f1, g1)\n",
        "        assert isinstance(res, np.ndarray)\n",
        "        success += 1\n",
        "    except:\n",
        "        print('Wrong type! Output is not of type np.ndarray')\n",
        "        fails += 1\n",
        "    try:\n",
        "        res = target(input_vector1, f1, g1)\n",
        "        assert np.allclose(res, expected1)\n",
        "        success += 1\n",
        "    except ValueError:\n",
        "        print('Error in test 1 \\nResult  : ', res, 'x \\nExpected: ', expected1)\n",
        "        fails += 1\n",
        "    try:\n",
        "        res = target(input_vector2, f2, g2)\n",
        "        assert np.allclose(res, expected2)\n",
        "        success += 1\n",
        "    except:\n",
        "        print('Error in test 2 \\nResult  : ', res, ' \\nExpected: ', expected2)\n",
        "        fails += 1\n",
        "            \n",
        "    if fails == 0:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print('\\033[92m', success,\" Tests passed\")\n",
        "        print('\\033[91m', fails, \" Tests failed\")\n",
        "        \n",
        "\n",
        "def test_ReformerLM(target):\n",
        "    test_cases = [\n",
        "                {\n",
        "                    \"name\":\"layer_len_check\",\n",
        "                    \"expected\":11,\n",
        "                    \"error\":\"We found {} layers in your model. It should be 11.\\nCheck the LSTM stack before the dense layer\"\n",
        "                },\n",
        "                {\n",
        "                    \"name\":\"simple_test_check\",\n",
        "      \"expected\":\"Serial[ShiftRight(1)Embedding_train_512DropoutPositionalEncodingDup_out2ReversibleSerial_in2_out2[ReversibleHalfResidualV2_in2_out2[Serial[LayerNorm]SelfAttention]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNormDense_2048DropoutFastGeluDense_512Dropout]]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNorm]SelfAttention]ReversibleSwap_in2_out2ReversibleHalfResidualV2_in2_out2[Serial[LayerNormDense_2048DropoutFastGeluDense_512Dropout]]ReversibleSwap_in2_out2]Concatenate_in2LayerNormDropoutDense_trainLogSoftmax]\",\n",
        "                    \"error\":\"The ReformerLM is not defined properly.\"\n",
        "                }\n",
        "            ]\n",
        "    temp_model = target('train')\n",
        "    \n",
        "    success = 0\n",
        "    fails = 0\n",
        "    \n",
        "    for test_case in test_cases:\n",
        "        try:\n",
        "            if test_case['name'] == \"simple_test_check\":\n",
        "                assert test_case[\"expected\"] == str(temp_model).replace(' ', '').replace('\\n','')\n",
        "                success += 1\n",
        "            if test_case['name'] == \"layer_len_check\":\n",
        "                if test_case[\"expected\"] == len(temp_model.sublayers):\n",
        "                    success += 1\n",
        "                else:\n",
        "                    print(test_case[\"error\"].format(len(temp_model.sublayers))) \n",
        "                    fails += 1\n",
        "        except:\n",
        "            print(test_case['error'])\n",
        "            fails += 1\n",
        "            \n",
        "    if fails == 0:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print('\\033[92m', success,\" Tests passed\")\n",
        "        print('\\033[91m', fails, \" Tests failed\")\n",
        "def test_tasks(train_task, eval_task):\n",
        "    target = train_task\n",
        "    success = 0\n",
        "    fails = 0     \n",
        "    try:\n",
        "        strlabel = str(target._labeled_data)\n",
        "        assert (\"generator\" in strlabel) and (\"add_loss_weights\" in  strlabel)\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong labeled data parameter in train_task\")    \n",
        "    try:\n",
        "        strlabel = str(target._loss_layer)\n",
        "        assert(strlabel == \"CrossEntropyLoss_in3\")\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong loss functions. CrossEntropyLoss_in3 was expected\")        \n",
        "    try:\n",
        "        assert(isinstance(target.optimizer, trax.optimizers.adam.Adam))\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong optimizer\")        \n",
        "    try:\n",
        "        assert(isinstance(target._lr_schedule,trax.supervised.lr_schedules._BodyAndTail))\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong learning rate schedule type\")    \n",
        "    try:\n",
        "        assert(target._n_steps_per_checkpoint==10)\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong checkpoint step frequency\")        \n",
        "    target = eval_task\n",
        "    try:\n",
        "        strlabel = str(target._labeled_data)\n",
        "        assert (\"generator\" in strlabel) and (\"add_loss_weights\" in  strlabel)\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(\"Wrong labeled data parameter in eval_task\")    \n",
        "    try:\n",
        "        strlabel = str(target._metrics).replace(' ', '')\n",
        "        assert(strlabel == \"[CrossEntropyLoss_in3,Accuracy_in3]\")\n",
        "        success += 1\n",
        "    except:\n",
        "        fails += 1\n",
        "        print(f\"Wrong metrics. found {strlabel} but expected [CrossEntropyLoss_in3,Accuracy_in3]\")                \n",
        "    if fails == 0:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print('\\033[92m', success,\" Tests passed\")\n",
        "        print('\\033[91m', fails, \" Tests failed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8lW48IGR9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01ee0bd-b74a-45b4-de62-c3a0961b0461"
      },
      "source": [
        "import w4_unittest\n",
        "w4_unittest.test_get_conversation(get_conversation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5_9rSeQQARw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff22021-1d1c-4829-a33f-af7a36927882"
      },
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "print(conversation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2EBvH-DQTIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc27b8bf-4ef3-46c8-dbe6-3a0537d64f7c"
      },
      "source": [
        "def print_conversation(conversation):\n",
        "    \n",
        "    delimiter_1 = 'Person 1: '\n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    split_list_d1 = conversation.split(delimiter_1)\n",
        "    \n",
        "    for sublist in split_list_d1[1:]:\n",
        "        split_list_d2 = sublist.split(delimiter_2)\n",
        "        print(colored(f'Person 1: {split_list_d2[0]}', 'red'))\n",
        "        \n",
        "        if len(split_list_d2) > 1:\n",
        "            print(colored(f'Person 2: {split_list_d2[1]}', 'green'))\n",
        "\n",
        "            \n",
        "print_conversation(conversation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPerson 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel \u001b[0m\n",
            "\u001b[32mPerson 2: Okay, do you have a specific area you want to stay in? \u001b[0m\n",
            "\u001b[31mPerson 1: no, i just need to make sure it's cheap. oh, and i need parking \u001b[0m\n",
            "\u001b[32mPerson 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? \u001b[0m\n",
            "\u001b[31mPerson 1: Yes, please. 6 people 3 nights starting on tuesday. \u001b[0m\n",
            "\u001b[32mPerson 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? \u001b[0m\n",
            "\u001b[31mPerson 1: how about only 2 nights. \u001b[0m\n",
            "\u001b[32mPerson 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? \u001b[0m\n",
            "\u001b[31mPerson 1: No, that will be all. Good bye. \u001b[0m\n",
            "\u001b[32mPerson 2: Thank you for using our services.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhtDG_9XQY2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e786ad35-8b49-45aa-ef19-3a0bdfeaa24d"
      },
      "source": [
        "DIALOGUE_DB['SNG01856.json']['log'][0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metadata': {},\n",
              " 'text': 'am looking for a place to to stay that has cheap price range it should be in a type of hotel'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNheU3eYQcuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ccbdf9-4f00-4f11-f1e3-8d8c23ddb3cd"
      },
      "source": [
        "attraction_file = open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/attraction_db.json')\n",
        "attractions = json.load(attraction_file)\n",
        "print(attractions[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'address': 'pool way, whitehill road, off newmarket road', 'area': 'east', 'entrance fee': '?', 'id': '1', 'location': [52.208789, 0.154883], 'name': 'abbey pool and astroturf pitch', 'openhours': '?', 'phone': '01223902088', 'postcode': 'cb58nt', 'pricerange': '?', 'type': 'swimmingpool'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ7bYp0LQgSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efca897-1448-4c69-a10e-cb45871cf5d6"
      },
      "source": [
        "hospital_file = open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/hospital_db.json')\n",
        "hospitals = json.load(hospital_file)\n",
        "print(hospitals[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'department': 'neurosciences critical care unit', 'id': 0, 'phone': '01223216297'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtIZ_-PcRfH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12aabce-4644-4d98-b072-f9821dfdf46b"
      },
      "source": [
        "hotel_file = open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/hotel_db.json')\n",
        "hotels = json.load(hotel_file)\n",
        "print(hotels[0]) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'address': '124 tenison road', 'area': 'east', 'internet': 'yes', 'parking': 'no', 'id': '0', 'location': [52.1963733, 0.1987426], 'name': 'a and b guest house', 'phone': '01223315702', 'postcode': 'cb12dp', 'price': {'double': '70', 'family': '90', 'single': '50'}, 'pricerange': 'moderate', 'stars': '4', 'takesbookings': 'yes', 'type': 'guesthouse'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59P45mACRrQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9980848-c8ab-46d2-a8dc-aa4b17a8a922"
      },
      "source": [
        "police_file = open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/police_db.json')\n",
        "police = json.load(police_file)\n",
        "print(police[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'Parkside Police Station', 'address': 'Parkside, Cambridge', 'id': 0, 'phone': '01223358966'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoTnTWNcSBYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f8de9a-ef3d-49f7-a8b4-8ef41bf3544d"
      },
      "source": [
        "restaurant_file = open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/restaurant_db.json')\n",
        "restaurants = json.load(restaurant_file)\n",
        "print(restaurants[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'address': 'Regent Street City Centre', 'area': 'centre', 'food': 'italian', 'id': '19210', 'introduction': 'Pizza hut is a large chain with restaurants nationwide offering convenience pizzas pasta and salads to eat in or take away', 'location': [52.20103, 0.126023], 'name': 'pizza hut city centre', 'phone': '01223323737', 'postcode': 'cb21ab', 'pricerange': 'cheap', 'type': 'restaurant'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqh_bPWoSUKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28763fc-2c1b-46d9-b33b-e46a1266ab3b"
      },
      "source": [
        "with open('/content/drive/MyDrive/Multilingual_ChatBot/Dataset/README.json') as file:\n",
        "    print(file.read())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#####################################################\n",
            "#####################################################\n",
            "#  Copyright Cambridge Dialogue Systems Group, 2018 #\n",
            "#####################################################\n",
            "#####################################################\n",
            "\n",
            "Dataset contains the following json files:\n",
            "1. data.json: the woz dialogue dataset, which contains the conversation  users and wizards, as well as a set of coarse labels for each user turn. Files with multi-domain dialogues have \"MUL\" in their names. Single domain dialogues have either \"SNG\" or \"WOZ\" in their names.\n",
            "2. restaurant_db.json: the Cambridge restaurant database file, containing restaurants in the Cambridge UK area and a set of attributes.\n",
            "3. attraction_db.json: the Cambridge attraction database file, contining attractions in the Cambridge UK area and a set of attributes.\n",
            "4. hotel_db.json: the Cambridge hotel database file, containing hotels in the Cambridge UK area and a set of attributes.\n",
            "5. train_db.json: the Cambridge train (with artificial connections) database file, containing trains in the Cambridge UK area and a set of attributes.\n",
            "6. hospital_db.json: the Cambridge hospital database file, contatining information about departments.\n",
            "7. police_db.json: the Cambridge police station information.\n",
            "8. taxi_db.json: slot-value list for taxi domain.\n",
            "9. valListFile.json: list of dialogues for validation.\n",
            "10. testListFile.json: list of dialogues for testing.\n",
            "11. system_acts.json:\n",
            "  There are 6 domains ('Booking', 'Restaurant', 'Hotel', 'Attraction', 'Taxi', 'Train') and 1 dummy domain ('general').\n",
            "  A domain-dependent dialogue act is defined as a domain token followed by a domain-independent dialogue act, e.g. 'Hotel-inform' means it is a 'inform' act in Hotel domain.\n",
            "  Dialogue acts which cannot take slots, e.g., 'good bye', are defined under 'general' domain.\n",
            "  A slot-value pair defined as a list with two elements. The first element is slot token and the second one is its value.\n",
            "  If a dialogue act takes no slots, e.g., dialogue act 'offer booking' for an utterance 'would you like to take a reservation?', its slot-value pair is ['none', 'none']\n",
            "  There are four types of value:\n",
            "  1) If a slot takes binary value, e.g., 'has Internet' or 'has park', the value is either 'yes' or 'no'.\n",
            "  2) If a slot is under the act 'request', e.g., 'request' about 'area', the value is express as '?'.\n",
            "  3) The value that appears in the utterancem e,g., the name of a restaurant.\n",
            "  4) If for some reasons the turn does not have annotation then it is labeled as \"No Annotation\".\n",
            "12. ontology.json: Data-based ontology.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLltCI4fSpOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8ae0b1-606c-4524-c0fe-3b39afc5e882"
      },
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "untokenized_data = []\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)    \n",
        "    untokenized_data.append(result)\n",
        "print(untokenized_data[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Person 1: am looking for a place to to stay that has cheap price range it should be in a type of hotel Person 2: Okay, do you have a specific area you want to stay in? Person 1: no, i just need to make sure it's cheap. oh, and i need parking Person 2: I found 1 cheap hotel for you that includes parking. Do you like me to book it? Person 1: Yes, please. 6 people 3 nights starting on tuesday. Person 2: I am sorry but I wasn't able to book that for you for Tuesday. Is there another day you would like to stay or perhaps a shorter stay? Person 1: how about only 2 nights. Person 2: Booking was successful.\n",
            "Reference number is : 7GAWK763. Anything else I can do for you? Person 1: No, that will be all. Good bye. Person 2: Thank you for using our services.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbzkgyDJSwmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45292f34-045a-4891-beb8-540084553a9d"
      },
      "source": [
        "random.shuffle(untokenized_data)\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of conversations in the data set: 10438\n",
            "number of conversations in train set: 9917\n",
            "number of conversations in eval set: 521\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPFz5-5eS1Ip"
      },
      "source": [
        "def stream(data):\n",
        "    while True:\n",
        "        d = random.choice(data)        \n",
        "        yield (d, d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BeL_-xaS5DY"
      },
      "source": [
        "data_pipeline = trax.data.Serial(\n",
        "    trax.data.Shuffle(),    \n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
        "                       vocab_file=VOCAB_FILE),    \n",
        "    trax.data.FilterByLength(2048),    \n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "        trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVTrWdNpS9QX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434b410e-e330-488f-eed1-45cbb5123eac"
      },
      "source": [
        "inp, _, _ = next(train_stream)\n",
        "print(\"input shape: \", inp.shape)\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape:  (4, 512)\n",
            " Person 1: I am looking for information and could use your help finding a train. Person 2: I'd be happy to help. Which day would you like to travel? Person 1: I need to leave stevenage on saturday. Person 2: I would be happy to help with your request, but first I will need to know what time you'd like to leave/arrive at. Person 1: I don't have a departure time preference but I would like to arrive by 15:45. Person 2: There are five trains to Cambridge on Saturday- the latest arrives at 14:43. Would you like me to book it for you? Person 1: What is the price for that one? Person 2: The cost for the train leaving Stevenage at 13:54, arriving Cambridge at 14:43 is 10.24 pounds, would you like me to book that for you today? Person 1: what's the train ID? Person 2: TR6607. Would you like me to book you on? Person 1: I also need a hotel. I don't care about parking but it does need free wifi.  Person 2: What area would you like to stay in? Person 1: the south . and  make it a guesthouse with free parking also.  Person 2: i have 3 guesthouses in the south. two are moderately priced and one is cheap. which do you choose? Person 1: Price is not an issue for me. Please recommend one of them for me. Person 2: Sure! I recommend Rosa's Bed and Breakfast. Would you like to book a room? Person 1: Yes please. I need a room for 8 and 2 nights starting from Saturday. Person 2: Booking was successful, the reference number is Y9ZD16I2, any further questions? Person 1: That is all I needed today.  Person 2: Thank you for calling. I hope you enjoy your trip to our town. Goodbye.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d7anSIK7YHZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY8aHid3TBOV"
      },
      "source": [
        "def reversible_layer_forward(x, f, g):\n",
        "    x1, x2 = np.split(x, 2, axis=-1)         \n",
        "    y1 = x1 + f(x2)    \n",
        "    y2 = x2 + g(y1)    \n",
        "    y = np.concatenate((y1,y2), axis = -1)    \n",
        "    return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfkwTyqZXZQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5132d3e-e4f6-4134-b98f-bfbafc683683"
      },
      "source": [
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "\n",
        "w4_unittest.test_reversible_layer_forward(reversible_layer_forward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Rgr2OdXb4V"
      },
      "source": [
        "def reversible_layer_reverse(y, f, g):\n",
        "    y1, y2 = np.split(y, 2, axis=-1)\n",
        "    x2 = y2 - g(y1)\n",
        "    x1 = y1 - f(x2)\n",
        "    x = np.concatenate((x1,x2),axis = -1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmGVmkbUXqMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7bee6c-26c8-40c0-c7b6-0d415f27f784"
      },
      "source": [
        "w4_unittest.test_reversible_layer_reverse(reversible_layer_reverse)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GI7pxD6Xu-V"
      },
      "source": [
        "f = lambda x: x + 2\n",
        "g = lambda x: x * 3\n",
        "input_vector = np.random.uniform(size=(32,))\n",
        "output_vector = reversible_layer_forward(input_vector, f, g)\n",
        "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
        "assert np.allclose(reversed_vector, input_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlKdcQZJXyGD"
      },
      "source": [
        "f = lambda x: x + np.random.uniform(size=x.shape)\n",
        "output_vector = reversible_layer_forward(input_vector, f, g)\n",
        "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
        "assert not np.allclose(reversed_vector, input_vector)  # Fails!!\n",
        "random_seed = 27686\n",
        "rng = trax.fastmath.random.get_prng(random_seed)\n",
        "f = lambda x: x + trax.fastmath.random.uniform(key=rng, shape=x.shape)\n",
        "output_vector = reversible_layer_forward(input_vector, f, g)\n",
        "reversed_vector = reversible_layer_reverse(output_vector, f, g)\n",
        "assert np.allclose(reversed_vector, input_vector,  atol=1e-07)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnT1ItmDX0_e"
      },
      "source": [
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = trax.models.reformer.ReformerLM( \n",
        "        vocab_size=vocab_size,\n",
        "        n_layers=n_layers,\n",
        "        mode=mode,\n",
        "        attention_type=attention_type\n",
        "    )    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsN-qIyiX6Ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f232a528-7a9d-480b-ce5d-f8ffbe50b5bb"
      },
      "source": [
        "temp_model = ReformerLM('train')\n",
        "print(str(temp_model))\n",
        "del temp_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_train_512\n",
            "  Dropout\n",
            "  Serial[\n",
            "    PositionalEncoding\n",
            "  ]\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderAttn_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidualDecoderFF_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        Serial[\n",
            "          FastGelu\n",
            "        ]\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Serial[\n",
            "    Dense_train\n",
            "  ]\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXjPgcseX8sP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bdae672-87c7-44ff-81ff-741c1fc7e1e3"
      },
      "source": [
        "w4_unittest.test_ReformerLM(ReformerLM)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We found 10 layers in your model. It should be 11.\n",
            "Check the LSTM stack before the dense layer\n",
            "The ReformerLM is not defined properly.\n",
            "\u001b[92m 0  Tests passed\n",
            "\u001b[91m 2  Tests failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPHve6CTIMfm"
      },
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"/content/drive/MyDrive/Multilingual_ChatBot/coursera-natural-language-processing-specialization/4 - Natural Language Processing with Attention Models/Week 4/model\"):\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
        "        n_warmup_steps=1000, max_value=0.01)\n",
        "    train_task = training.TrainTask(            \n",
        "        labeled_data=train_gen,\n",
        "        loss_layer=tl.CrossEntropyLoss(),\n",
        "        optimizer=trax.optimizers.Adam(0.01),\n",
        "        lr_schedule=lr_schedule,\n",
        "        n_steps_per_checkpoint=10\n",
        "    )\n",
        "    eval_task = training.EvalTask(                      \n",
        "        labeled_data=eval_gen,\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
        "    )\n",
        "\n",
        "    loop = training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    return loop\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRwpwj-DIxVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6f9558-8403-430a-b9f1-96a8cfd1237c"
      },
      "source": [
        "test_loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "train_task = test_loop.tasks\n",
        "eval_task = test_loop.eval_tasks\n",
        "\n",
        "print(train_task)\n",
        "print(eval_task)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:317: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[<trax.supervised.training.TrainTask object at 0x7ff3a462d510>]\n",
            "[<trax.supervised.training.EvalTask object at 0x7ff38a4beed0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bcNrvxwIyT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5b84dd-18f4-4a87-84de-615cd27424b7"
      },
      "source": [
        "w4_unittest.test_tasks(train_task, eval_task)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong labeled data parameter in train_task\n",
            "Wrong loss functions. CrossEntropyLoss_in3 was expected\n",
            "Wrong optimizer\n",
            "Wrong learning rate schedule type\n",
            "Wrong checkpoint step frequency\n",
            "Wrong labeled data parameter in eval_task\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xGFZ3XxJPje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5eeb28e-2ec1-4e6f-8cb1-f0dfdd720737"
      },
      "source": [
        "!rm -f model/model.pkl.gz\n",
        "loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "loop.run(5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:304: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n",
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:317: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsmhQC93oydi"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=7,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n",
        "shape11 = trax.shapes.ShapeDtype((11, 11), dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vefg1ah4pJmp"
      },
      "source": [
        "#model.init_from_file('/content/drive/MyDrive/Multilingual_ChatBot/coursera-natural-language-processing-specialization/4 - Natural Language Processing with Attention Models/Week 4/model/model.pkl.gz',\n",
        "      #               weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "#STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "img43Q5JJNgv"
      },
      "source": [
        "STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6PR3xqmpMsX"
      },
      "source": [
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Fsdbh0soSV"
      },
      "source": [
        "#model.init_from_file('model/model.pkl.gz',\n",
        "#                     weights_only=True, input_signature=shape11)\n",
        "#STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSxe8vNTssWJ"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9W4JIins8jh"
      },
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isw9tw13s_9i"
      },
      "source": [
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        ReformerLM,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    return output_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XljtsN9jtDpc"
      },
      "source": [
        "#import pickle\n",
        "#WEIGHTS_FROM_FILE = ()\n",
        "#with open('/content/drive/MyDrive/Multilingual_ChatBot/coursera-natural-language-processing-specialization/4 - Natural Language Processing with Attention Models/Week 4/weights', 'rb') as file:\n",
        "#    WEIGHTS_FROM_FILE = pickle.load(file)\n",
        "#shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "#def attention(*args, **kwargs):\n",
        "#    kwargs['predict_mem_len'] = 120\n",
        "#   kwargs['predict_drop_len'] = 120\n",
        "#    return tl.SelfAttention(*args, **kwargs)\n",
        "#test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)\n",
        "#3test_output_gen = ReformerLM_output_gen(test_model, \"test\", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)\n",
        "#test_model.init_weights_and_state(shape11)\n",
        "#test_model.weights = WEIGHTS_FROM_FILE\n",
        "#output = []\n",
        "#for i in range(6):\n",
        "#    output.append(next(test_output_gen)[0])\n",
        "#print(output)\n",
        "#del test_model \n",
        "#del WEIGHTS_FROM_FILE\n",
        "#del test_output_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sccMvWxYtHl9"
      },
      "source": [
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWSIZiKFthw6"
      },
      "source": [
        "#model.init_from_file('model/model.pkl.gz',\n",
        "#                     weights_only=True, input_signature=shape11)\n",
        "#STARTING_STATE = model.state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ14cH8atogl"
      },
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    sentence = ''\n",
        "    counter = 0\n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    ReformerLM.state = model_state\n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    for o in output:\n",
        "        result.append(o)\n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        counter += 1        \n",
        "        if counter > max_len:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4X3ajSStr_G"
      },
      "source": [
        "#sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "#generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYWMq7SPtvBS"
      },
      "source": [
        "#sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "#generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWRcdXbjty2l"
      },
      "source": [
        "#sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "#generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoNr2prYuvss"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jy_irc2lKS2",
        "outputId": "611d9af7-3cbd-48c9-a322-688452377beb"
      },
      "source": [
        "!git clone https://github.com/parulnith/Building-a-Simple-Chatbot-in-Python-using-NLTK"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Building-a-Simple-Chatbot-in-Python-using-NLTK'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Total 95 (delta 0), reused 0 (delta 0), pack-reused 95\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnai4QGLlryO"
      },
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsBKlbWvmWUi",
        "outputId": "0aa78ee5-d0f2-4cf4-9307-dbcb1b4d2a3e"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYKeHGtTmbCc"
      },
      "source": [
        "f=open('/content/Building-a-Simple-Chatbot-in-Python-using-NLTK/chatbot.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw = raw.lower()# converts to lowercase\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTO7U-PSnwvi",
        "outputId": "08475c21-e7f5-44bd-ff3a-07ab4249c5e3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKuqP2VjnSko"
      },
      "source": [
        "\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBbeUuPQncWj"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj6Ak189n4uW"
      },
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYfG5RFKn72e"
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUfGUxQroMkv",
        "outputId": "c21b4a93-45e2-4bd5-c62c-b25bd4cffa4c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H33O_a3n_QG",
        "outputId": "be612bf3-b3f4-4ad8-ecb2-36581ce0faab"
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU7L0MfFoCr2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}