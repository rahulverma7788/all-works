{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment1bilstmgl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrPP_6XJEI6d",
        "outputId": "343b2cbf-313f-4803-97cc-58a101f7097e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6b0VZ3KHu08",
        "outputId": "2bb4e085-3834-41c3-f38f-7e37d12dfc4b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import textblob\n",
        "from textblob import TextBlob, Word\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.stem import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer()\n",
        "from textblob import TextBlob\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHAZauvvESGw"
      },
      "source": [
        "def read_dataset():\n",
        "    data = pd.read_csv(r\"/content/drive/MyDrive/data.csv\", sep = '\\t', header = None)\n",
        "    data.columns = ['id', 'sentence']\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQO3PFB7ESJC"
      },
      "source": [
        "def sample_data(data, sample_value):\n",
        "    data = data[:][:sample_value]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI7SOjJDESLG",
        "outputId": "d9c1a05a-b2c7-4f10-efb3-6a51535ccc29"
      },
      "source": [
        "print(read_dataset())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id                                           sentence\n",
            "0            1  I always waited for things to get better. I al...\n",
            "1            2  I’ve had rope under my dresser for a while now...\n",
            "2            3  I’m just burnt out I have nothing left. My gf ...\n",
            "3            4  I want to commit suicide.. I feel like I have ...\n",
            "4            5  It's like being thrown in a pit of suffering t...\n",
            "...        ...                                                ...\n",
            "122272  122273  Hey y'all. Not really sure what I'm doing but ...\n",
            "122273  122274  I’m just wondering if there’s anyone like me t...\n",
            "122274  122275  Amber you don't matter no one loves you. Why a...\n",
            "122275  122276  Title explains it. I am going to sleep, and I ...\n",
            "122276  122277  If you look at my old post then you’ll see wha...\n",
            "\n",
            "[122277 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj7URsgZESNd"
      },
      "source": [
        "data = read_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5jmkl86ESQm",
        "outputId": "ebb2bd16-f316-44db-c341-a8e59f60944b"
      },
      "source": [
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id                                           sentence\n",
            "0            1  I always waited for things to get better. I al...\n",
            "1            2  I’ve had rope under my dresser for a while now...\n",
            "2            3  I’m just burnt out I have nothing left. My gf ...\n",
            "3            4  I want to commit suicide.. I feel like I have ...\n",
            "4            5  It's like being thrown in a pit of suffering t...\n",
            "...        ...                                                ...\n",
            "122272  122273  Hey y'all. Not really sure what I'm doing but ...\n",
            "122273  122274  I’m just wondering if there’s anyone like me t...\n",
            "122274  122275  Amber you don't matter no one loves you. Why a...\n",
            "122275  122276  Title explains it. I am going to sleep, and I ...\n",
            "122276  122277  If you look at my old post then you’ll see wha...\n",
            "\n",
            "[122277 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agFbH93nESSp"
      },
      "source": [
        "def avg_word_len (sentence):\n",
        "    words = sentence.split()\n",
        "    avg_len = sum(len(word) for word in words)/len(words)\n",
        "    return avg_len\n",
        "\n",
        "def extract_ngrams(data, num):\n",
        "    '''\n",
        "    Function to generate n-grams from sentences\n",
        "    '''\n",
        "    n_grams = TextBlob(data).ngrams(num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IWcnvjBNESVl",
        "outputId": "60b1a862-7438-4663-a454-cc3b89677d9e"
      },
      "source": [
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(122277, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I always waited for things to get better. I al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>I’ve had rope under my dresser for a while now...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>I’m just burnt out I have nothing left. My gf ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>I want to commit suicide.. I feel like I have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>It's like being thrown in a pit of suffering t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           sentence\n",
              "0   1  I always waited for things to get better. I al...\n",
              "1   2  I’ve had rope under my dresser for a while now...\n",
              "2   3  I’m just burnt out I have nothing left. My gf ...\n",
              "3   4  I want to commit suicide.. I feel like I have ...\n",
              "4   5  It's like being thrown in a pit of suffering t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "CNKGURo6ESYA",
        "outputId": "980e0428-5fb8-4fa7-d089-0b747ba19286"
      },
      "source": [
        "data['char_count'] = data['sentence'].str.len()\n",
        "df_train_sort_charcount = data.sort_values(by='char_count', ascending=False)\n",
        "df_train_sort_charcount[['sentence', 'char_count']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6473</th>\n",
              "      <td>https://www.evernote.com/shard/s405/sh/cb9e84c...</td>\n",
              "      <td>39883.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44562</th>\n",
              "      <td>When I started writing this, I didn't realize ...</td>\n",
              "      <td>39384.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8570</th>\n",
              "      <td>Get out of my head. Get out of my head. Get ou...</td>\n",
              "      <td>38719.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39102</th>\n",
              "      <td>片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の...</td>\n",
              "      <td>38287.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93875</th>\n",
              "      <td>Some may remember me. I was asked what’s going...</td>\n",
              "      <td>37055.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  char_count\n",
              "6473   https://www.evernote.com/shard/s405/sh/cb9e84c...     39883.0\n",
              "44562  When I started writing this, I didn't realize ...     39384.0\n",
              "8570   Get out of my head. Get out of my head. Get ou...     38719.0\n",
              "39102   片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の...     38287.0\n",
              "93875  Some may remember me. I was asked what’s going...     37055.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "szMMELgoFCDv",
        "outputId": "36e1eac1-1a27-4982-86eb-ad7566c5fa92"
      },
      "source": [
        "data['word_count'] = data['sentence'].apply(lambda x: len(str(x).split(\" \")))\n",
        "df_train_sort_wordcount = data.sort_values(by='word_count', ascending=False)\n",
        "df_train_sort_wordcount[['sentence','word_count']].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39102</th>\n",
              "      <td>片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の...</td>\n",
              "      <td>10940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8570</th>\n",
              "      <td>Get out of my head. Get out of my head. Get ou...</td>\n",
              "      <td>9680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44562</th>\n",
              "      <td>When I started writing this, I didn't realize ...</td>\n",
              "      <td>7636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6473</th>\n",
              "      <td>https://www.evernote.com/shard/s405/sh/cb9e84c...</td>\n",
              "      <td>7115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94633</th>\n",
              "      <td>My name is Richard. I’m 28 years old. I was bo...</td>\n",
              "      <td>7088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  word_count\n",
              "39102   片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の天使  片翼の...       10940\n",
              "8570   Get out of my head. Get out of my head. Get ou...        9680\n",
              "44562  When I started writing this, I didn't realize ...        7636\n",
              "6473   https://www.evernote.com/shard/s405/sh/cb9e84c...        7115\n",
              "94633  My name is Richard. I’m 28 years old. I was bo...        7088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH_ky7mQFCGB",
        "outputId": "2d76fa05-1026-4caa-e609-d2e50f59a3f1"
      },
      "source": [
        "data = data['sentence'][0]\n",
        " \n",
        "print(\"1-gram: \", extract_ngrams(data, 1))\n",
        "print(\"2-gram: \", extract_ngrams(data, 2))\n",
        "print(\"3-gram: \", extract_ngrams(data, 3))\n",
        "print(\"4-gram: \", extract_ngrams(data, 4))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-gram:  ['I', 'always', 'waited', 'for', 'things', 'to', 'get', 'better', 'I', 'always', 'waited', 'for', 'things', 'to', 'improve', 'and', 'people', 'to', 'change', 'and', 'for', 'myself', 'to', 'be', 'happy', 'I', '’', 'm', 'so', 'tired', 'of', 'waiting', 'Nothing', 'is', 'going', 'to', 'get', 'better', 'for', 'me', 'anyways', 'My', 'family', 'is', 'still', 'gon', 'na', 'be', 'shitty', 'tomorrow', 'My', 'past', 'isn', '’', 't', 'gon', 'na', 'change', 'tomorrow', 'My', 'brain', 'isn', '’', 't', 'going', 'to', 'change', 'tomorrow', 'The', 'way', 'I', '’', 'm', 'so', 'extremely', 'defective', 'and', 'unloveable', 'isn', '’', 't', 'going', 'to', 'change', 'tomorrow', 'And', 'the', 'tomorrow', '’', 's', 'are', 'interchangeable', 'for', 'any', 'range', 'of', 'time', 'as', 'I', '’', 've', 'been', 'waiting', 'for', 'all', 'of', 'this', 'for', 'years', 'now', 'I', 'can', '’', 't', 'remember', 'a', 'time', 'where', 'I', 'didn', '’', 't', 'want', 'to', 'give', 'up', 'I', 'just', 'am', 'so', 'sick', 'of', 'this', 'I', 'don', '’', 't', 'know', 'what', 'I', 'did', 'to', 'deserve', 'it', 'all', 'No', 'matter', 'what', 'I', 'do', 'it', '’', 's', 'not', 'good', 'enough', 'and', 'I', '’', 'm', 'not', 'gon', 'na', 'be', 'okay', 'ever', 'I', 'can', '’', 't', 'see', 'myself', 'ever', 'being', 'older', 'than', 'this', 'I', 'don', '’', 't', 'want', 'to', 'do', 'it', 'anymore']\n",
            "2-gram:  ['I always', 'always waited', 'waited for', 'for things', 'things to', 'to get', 'get better', 'better I', 'I always', 'always waited', 'waited for', 'for things', 'things to', 'to improve', 'improve and', 'and people', 'people to', 'to change', 'change and', 'and for', 'for myself', 'myself to', 'to be', 'be happy', 'happy I', 'I ’', '’ m', 'm so', 'so tired', 'tired of', 'of waiting', 'waiting Nothing', 'Nothing is', 'is going', 'going to', 'to get', 'get better', 'better for', 'for me', 'me anyways', 'anyways My', 'My family', 'family is', 'is still', 'still gon', 'gon na', 'na be', 'be shitty', 'shitty tomorrow', 'tomorrow My', 'My past', 'past isn', 'isn ’', '’ t', 't gon', 'gon na', 'na change', 'change tomorrow', 'tomorrow My', 'My brain', 'brain isn', 'isn ’', '’ t', 't going', 'going to', 'to change', 'change tomorrow', 'tomorrow The', 'The way', 'way I', 'I ’', '’ m', 'm so', 'so extremely', 'extremely defective', 'defective and', 'and unloveable', 'unloveable isn', 'isn ’', '’ t', 't going', 'going to', 'to change', 'change tomorrow', 'tomorrow And', 'And the', 'the tomorrow', 'tomorrow ’', '’ s', 's are', 'are interchangeable', 'interchangeable for', 'for any', 'any range', 'range of', 'of time', 'time as', 'as I', 'I ’', '’ ve', 've been', 'been waiting', 'waiting for', 'for all', 'all of', 'of this', 'this for', 'for years', 'years now', 'now I', 'I can', 'can ’', '’ t', 't remember', 'remember a', 'a time', 'time where', 'where I', 'I didn', 'didn ’', '’ t', 't want', 'want to', 'to give', 'give up', 'up I', 'I just', 'just am', 'am so', 'so sick', 'sick of', 'of this', 'this I', 'I don', 'don ’', '’ t', 't know', 'know what', 'what I', 'I did', 'did to', 'to deserve', 'deserve it', 'it all', 'all No', 'No matter', 'matter what', 'what I', 'I do', 'do it', 'it ’', '’ s', 's not', 'not good', 'good enough', 'enough and', 'and I', 'I ’', '’ m', 'm not', 'not gon', 'gon na', 'na be', 'be okay', 'okay ever', 'ever I', 'I can', 'can ’', '’ t', 't see', 'see myself', 'myself ever', 'ever being', 'being older', 'older than', 'than this', 'this I', 'I don', 'don ’', '’ t', 't want', 'want to', 'to do', 'do it', 'it anymore']\n",
            "3-gram:  ['I always waited', 'always waited for', 'waited for things', 'for things to', 'things to get', 'to get better', 'get better I', 'better I always', 'I always waited', 'always waited for', 'waited for things', 'for things to', 'things to improve', 'to improve and', 'improve and people', 'and people to', 'people to change', 'to change and', 'change and for', 'and for myself', 'for myself to', 'myself to be', 'to be happy', 'be happy I', 'happy I ’', 'I ’ m', '’ m so', 'm so tired', 'so tired of', 'tired of waiting', 'of waiting Nothing', 'waiting Nothing is', 'Nothing is going', 'is going to', 'going to get', 'to get better', 'get better for', 'better for me', 'for me anyways', 'me anyways My', 'anyways My family', 'My family is', 'family is still', 'is still gon', 'still gon na', 'gon na be', 'na be shitty', 'be shitty tomorrow', 'shitty tomorrow My', 'tomorrow My past', 'My past isn', 'past isn ’', 'isn ’ t', '’ t gon', 't gon na', 'gon na change', 'na change tomorrow', 'change tomorrow My', 'tomorrow My brain', 'My brain isn', 'brain isn ’', 'isn ’ t', '’ t going', 't going to', 'going to change', 'to change tomorrow', 'change tomorrow The', 'tomorrow The way', 'The way I', 'way I ’', 'I ’ m', '’ m so', 'm so extremely', 'so extremely defective', 'extremely defective and', 'defective and unloveable', 'and unloveable isn', 'unloveable isn ’', 'isn ’ t', '’ t going', 't going to', 'going to change', 'to change tomorrow', 'change tomorrow And', 'tomorrow And the', 'And the tomorrow', 'the tomorrow ’', 'tomorrow ’ s', '’ s are', 's are interchangeable', 'are interchangeable for', 'interchangeable for any', 'for any range', 'any range of', 'range of time', 'of time as', 'time as I', 'as I ’', 'I ’ ve', '’ ve been', 've been waiting', 'been waiting for', 'waiting for all', 'for all of', 'all of this', 'of this for', 'this for years', 'for years now', 'years now I', 'now I can', 'I can ’', 'can ’ t', '’ t remember', 't remember a', 'remember a time', 'a time where', 'time where I', 'where I didn', 'I didn ’', 'didn ’ t', '’ t want', 't want to', 'want to give', 'to give up', 'give up I', 'up I just', 'I just am', 'just am so', 'am so sick', 'so sick of', 'sick of this', 'of this I', 'this I don', 'I don ’', 'don ’ t', '’ t know', 't know what', 'know what I', 'what I did', 'I did to', 'did to deserve', 'to deserve it', 'deserve it all', 'it all No', 'all No matter', 'No matter what', 'matter what I', 'what I do', 'I do it', 'do it ’', 'it ’ s', '’ s not', 's not good', 'not good enough', 'good enough and', 'enough and I', 'and I ’', 'I ’ m', '’ m not', 'm not gon', 'not gon na', 'gon na be', 'na be okay', 'be okay ever', 'okay ever I', 'ever I can', 'I can ’', 'can ’ t', '’ t see', 't see myself', 'see myself ever', 'myself ever being', 'ever being older', 'being older than', 'older than this', 'than this I', 'this I don', 'I don ’', 'don ’ t', '’ t want', 't want to', 'want to do', 'to do it', 'do it anymore']\n",
            "4-gram:  ['I always waited for', 'always waited for things', 'waited for things to', 'for things to get', 'things to get better', 'to get better I', 'get better I always', 'better I always waited', 'I always waited for', 'always waited for things', 'waited for things to', 'for things to improve', 'things to improve and', 'to improve and people', 'improve and people to', 'and people to change', 'people to change and', 'to change and for', 'change and for myself', 'and for myself to', 'for myself to be', 'myself to be happy', 'to be happy I', 'be happy I ’', 'happy I ’ m', 'I ’ m so', '’ m so tired', 'm so tired of', 'so tired of waiting', 'tired of waiting Nothing', 'of waiting Nothing is', 'waiting Nothing is going', 'Nothing is going to', 'is going to get', 'going to get better', 'to get better for', 'get better for me', 'better for me anyways', 'for me anyways My', 'me anyways My family', 'anyways My family is', 'My family is still', 'family is still gon', 'is still gon na', 'still gon na be', 'gon na be shitty', 'na be shitty tomorrow', 'be shitty tomorrow My', 'shitty tomorrow My past', 'tomorrow My past isn', 'My past isn ’', 'past isn ’ t', 'isn ’ t gon', '’ t gon na', 't gon na change', 'gon na change tomorrow', 'na change tomorrow My', 'change tomorrow My brain', 'tomorrow My brain isn', 'My brain isn ’', 'brain isn ’ t', 'isn ’ t going', '’ t going to', 't going to change', 'going to change tomorrow', 'to change tomorrow The', 'change tomorrow The way', 'tomorrow The way I', 'The way I ’', 'way I ’ m', 'I ’ m so', '’ m so extremely', 'm so extremely defective', 'so extremely defective and', 'extremely defective and unloveable', 'defective and unloveable isn', 'and unloveable isn ’', 'unloveable isn ’ t', 'isn ’ t going', '’ t going to', 't going to change', 'going to change tomorrow', 'to change tomorrow And', 'change tomorrow And the', 'tomorrow And the tomorrow', 'And the tomorrow ’', 'the tomorrow ’ s', 'tomorrow ’ s are', '’ s are interchangeable', 's are interchangeable for', 'are interchangeable for any', 'interchangeable for any range', 'for any range of', 'any range of time', 'range of time as', 'of time as I', 'time as I ’', 'as I ’ ve', 'I ’ ve been', '’ ve been waiting', 've been waiting for', 'been waiting for all', 'waiting for all of', 'for all of this', 'all of this for', 'of this for years', 'this for years now', 'for years now I', 'years now I can', 'now I can ’', 'I can ’ t', 'can ’ t remember', '’ t remember a', 't remember a time', 'remember a time where', 'a time where I', 'time where I didn', 'where I didn ’', 'I didn ’ t', 'didn ’ t want', '’ t want to', 't want to give', 'want to give up', 'to give up I', 'give up I just', 'up I just am', 'I just am so', 'just am so sick', 'am so sick of', 'so sick of this', 'sick of this I', 'of this I don', 'this I don ’', 'I don ’ t', 'don ’ t know', '’ t know what', 't know what I', 'know what I did', 'what I did to', 'I did to deserve', 'did to deserve it', 'to deserve it all', 'deserve it all No', 'it all No matter', 'all No matter what', 'No matter what I', 'matter what I do', 'what I do it', 'I do it ’', 'do it ’ s', 'it ’ s not', '’ s not good', 's not good enough', 'not good enough and', 'good enough and I', 'enough and I ’', 'and I ’ m', 'I ’ m not', '’ m not gon', 'm not gon na', 'not gon na be', 'gon na be okay', 'na be okay ever', 'be okay ever I', 'okay ever I can', 'ever I can ’', 'I can ’ t', 'can ’ t see', '’ t see myself', 't see myself ever', 'see myself ever being', 'myself ever being older', 'ever being older than', 'being older than this', 'older than this I', 'than this I don', 'this I don ’', 'I don ’ t', 'don ’ t want', '’ t want to', 't want to do', 'want to do it', 'to do it anymore']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnmvjulqFCIy"
      },
      "source": [
        "def convert_to_string (df_train):\n",
        "    for i in range(len(df_train)):\n",
        "        df_train['sentence'][i] = str(df_train['sentence'][i])\n",
        "    return data\n",
        "\n",
        "def data_processing(data):\n",
        "    for i in range(len(data)):\n",
        "        # Remove Excape Sequences\n",
        "        sentence = remove_excape_sequesces(data['sentence'][i])\n",
        "        # Remove Links\n",
        "        sentence = remove_links(sentence)\n",
        "        # Forming the final paragraphs\n",
        "        senetnce = form_paragraph(sentence)\n",
        "        # Perform further NLP processing\n",
        "        sentence = apply_NLP(sentence)\n",
        "\n",
        "        data['sentence'][i] = sentence\n",
        "    return data\n",
        "\n",
        "def apply_NLP (sentence):\n",
        "    sentences = sentence.split(sep = '.')\n",
        "    for i in range(len(sentences)):\n",
        "        # Tokenize the words\n",
        "        tokens = word_tokenize(sentences[i])\n",
        "        tokens = [word.lower() for word in tokens]\n",
        "        # Remove puntuations\n",
        "        no_punctuations = [word.translate(table) for word in tokens]\n",
        "        # Remove all non-alphabetic characters.\n",
        "        words = [word for word in no_punctuations if word.isalpha()]\n",
        "        # Remove Stop words and convert the sentence to its base form\n",
        "        words = [word_lem.lemmatize(w) for w in words if not w in stop_words]\n",
        "        sentences[i] = \" \".join(words)\n",
        "    sentence = \" .\".join(sentences)\n",
        "    return sentence+\".\"\n",
        "\n",
        "def remove_excape_sequesces (sentence):\n",
        "    excape_sequences = ['\\n', '\\t', '\\r', '\\\\', '\\a', '\\f', '\\o', '\\v', '\\b', '\\\"', '\\'', '\\newline']\n",
        "    temp = \"\"\n",
        "    sentence = str(sentence)\n",
        "    for i in sentence:\n",
        "        if i in excape_sequences:\n",
        "            continue\n",
        "        temp += i\n",
        "    return temp\n",
        "\n",
        "def remove_links (sentence):\n",
        "    comment = sentence.split(sep = \" \")\n",
        "    for word in comment:\n",
        "        if ('http' in word) or ('www' in word) or ('https' in word) or (len(word) == 0):\n",
        "            comment.remove(word)\n",
        "    sentence = \" \".join(comment)\n",
        "    return sentence\n",
        "\n",
        "def form_paragraph (sentence):\n",
        "    temp = nltk.sent_tokenize(sentence)\n",
        "    for index in range(len(temp)):\n",
        "        temp[index] = temp[index].replace('.', ' ')\n",
        "    for index in range(len(temp)):\n",
        "        d = []\n",
        "        k = temp[index].split(sep = ' ')\n",
        "        for z in range(len(k)):\n",
        "            if len(k[z]) > 0:\n",
        "                d.append(k[z]) \n",
        "        s = ''\n",
        "        for j in range(len(d)):\n",
        "            s = s + ' ' + d[j].strip()\n",
        "        temp[index] = s.strip()      \n",
        "    sentence = '. '.join(temp).lower()\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ox5FQwESab"
      },
      "source": [
        "def polarity_and_subjectivity(data):\n",
        "    polarity_sentence  = []\n",
        "    subjectivity_sentence = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        \n",
        "        # create temp variables\n",
        "        sentence = data['sentence'][i]\n",
        "        \n",
        "        # form object and calculate polarity and subjectivity of sentence\n",
        "        obj_sentence = TextBlob(sentence)\n",
        "        polarity = obj_sentence.sentiment.polarity\n",
        "        subjectivity = obj_sentence.sentiment.subjectivity\n",
        "        \n",
        "        # save the polarity and subjectivity of sentence\n",
        "        polarity_sentence.append(polarity)\n",
        "        subjectivity_sentence.append(subjectivity)\n",
        "\n",
        "    data['polarity'] = polarity_sentence\n",
        "    data['subjectivity'] = subjectivity_sentence\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIK-6XhYESc5"
      },
      "source": [
        "def filter_polarity_and_subjectivity (data):\n",
        "    k = []\n",
        "    for i in range(len(data)):\n",
        "        if data['subjectivity'][i] <= subjectivity_threshold and len(str(data['sentence'][i])) > 10:\n",
        "            k.append(data['id'][i])\n",
        "    data = data.drop(np.array(k)-1)\n",
        "    labels = []\n",
        "    data = data.reset_index(drop=True)\n",
        "    for i in range(len(data)):\n",
        "        if data['polarity'][i] < polarity_threshold:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "    data['labels'] = labels\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "DCLMomOzESfS",
        "outputId": "430a1dbf-08f2-41e5-d207-a6bf1ae4a070"
      },
      "source": [
        "data = read_dataset()\n",
        "data = sample_data(data, 110000)\n",
        "data = convert_to_string(data)\n",
        "data = polarity_and_subjectivity(data)\n",
        "polarity_threshold = 0          # range(-1 to +1)\n",
        "subjectivity_threshold = 0.5    # range(0 to 1)\n",
        "data = filter_polarity_and_subjectivity(data)\n",
        "data = data_processing(data)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>always waited thing get better .always waited ...</td>\n",
              "      <td>0.057035</td>\n",
              "      <td>0.612771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>tonight night dont care fucking hurt . . . ..</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>im tucking tired people pretend care say theyr...</td>\n",
              "      <td>0.123889</td>\n",
              "      <td>0.582778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>sucidal intention turned cutting stop many cut...</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.558333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>slowly becoming mentally unstable show.</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63478</th>\n",
              "      <td>109993</td>\n",
              "      <td>hear story people fail committing suicide they...</td>\n",
              "      <td>-0.219852</td>\n",
              "      <td>0.615051</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63479</th>\n",
              "      <td>109994</td>\n",
              "      <td>ughhhmy life fine part im average hate alive h...</td>\n",
              "      <td>-0.114815</td>\n",
              "      <td>0.547222</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63480</th>\n",
              "      <td>109997</td>\n",
              "      <td>sick mind game .sick ugly girl .sick alone .si...</td>\n",
              "      <td>-0.447403</td>\n",
              "      <td>0.707792</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63481</th>\n",
              "      <td>109998</td>\n",
              "      <td>year old male would say good looking even tho ...</td>\n",
              "      <td>0.051580</td>\n",
              "      <td>0.539011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63482</th>\n",
              "      <td>109999</td>\n",
              "      <td>cant stop thinking incest whenever sex subject...</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>0.575833</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63483 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... labels\n",
              "0           1  ...      0\n",
              "1           6  ...      1\n",
              "2           7  ...      0\n",
              "3           9  ...      0\n",
              "4          10  ...      0\n",
              "...       ...  ...    ...\n",
              "63478  109993  ...      1\n",
              "63479  109994  ...      1\n",
              "63480  109997  ...      1\n",
              "63481  109998  ...      0\n",
              "63482  109999  ...      1\n",
              "\n",
              "[63483 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "tQ5uKzLaFpYI",
        "outputId": "7b514a6c-01f6-4a17-f196-6866832c2231"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>always waited thing get better .always waited ...</td>\n",
              "      <td>0.057035</td>\n",
              "      <td>0.612771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>tonight night dont care fucking hurt . . . ..</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>im tucking tired people pretend care say theyr...</td>\n",
              "      <td>0.123889</td>\n",
              "      <td>0.582778</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>sucidal intention turned cutting stop many cut...</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.558333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>slowly becoming mentally unstable show.</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63478</th>\n",
              "      <td>109993</td>\n",
              "      <td>hear story people fail committing suicide they...</td>\n",
              "      <td>-0.219852</td>\n",
              "      <td>0.615051</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63479</th>\n",
              "      <td>109994</td>\n",
              "      <td>ughhhmy life fine part im average hate alive h...</td>\n",
              "      <td>-0.114815</td>\n",
              "      <td>0.547222</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63480</th>\n",
              "      <td>109997</td>\n",
              "      <td>sick mind game .sick ugly girl .sick alone .si...</td>\n",
              "      <td>-0.447403</td>\n",
              "      <td>0.707792</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63481</th>\n",
              "      <td>109998</td>\n",
              "      <td>year old male would say good looking even tho ...</td>\n",
              "      <td>0.051580</td>\n",
              "      <td>0.539011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63482</th>\n",
              "      <td>109999</td>\n",
              "      <td>cant stop thinking incest whenever sex subject...</td>\n",
              "      <td>-0.143667</td>\n",
              "      <td>0.575833</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63483 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  ... labels\n",
              "0           1  ...      0\n",
              "1           6  ...      1\n",
              "2           7  ...      0\n",
              "3           9  ...      0\n",
              "4          10  ...      0\n",
              "...       ...  ...    ...\n",
              "63478  109993  ...      1\n",
              "63479  109994  ...      1\n",
              "63480  109997  ...      1\n",
              "63481  109998  ...      0\n",
              "63482  109999  ...      1\n",
              "\n",
              "[63483 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02E0VjXVFpaR",
        "outputId": "ff0f1599-a96d-4d6f-9f42-f4cbe6521dc0"
      },
      "source": [
        "data.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63483, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "9JPnHeI-Fpcd",
        "outputId": "28ad2d51-ffb7-4a71-8982-2a48fffac6df"
      },
      "source": [
        "data.sample(10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20179</th>\n",
              "      <td>34855</td>\n",
              "      <td>remember last time truly happy .feel like talk...</td>\n",
              "      <td>0.108519</td>\n",
              "      <td>0.510370</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32455</th>\n",
              "      <td>56355</td>\n",
              "      <td>try think reason get bed .give info able walk ...</td>\n",
              "      <td>0.029221</td>\n",
              "      <td>0.589069</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4445</th>\n",
              "      <td>7780</td>\n",
              "      <td>dated month .im fucked relationship .cant even...</td>\n",
              "      <td>0.102424</td>\n",
              "      <td>0.555000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9736</th>\n",
              "      <td>16947</td>\n",
              "      <td>gun rope im teen cant drive go anywhere .im we...</td>\n",
              "      <td>0.056250</td>\n",
              "      <td>0.656250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21736</th>\n",
              "      <td>37566</td>\n",
              "      <td>im locked room staring bland wall .maybe want ...</td>\n",
              "      <td>-0.078333</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16908</th>\n",
              "      <td>29308</td>\n",
              "      <td>came made resolution go flow optimistic .final...</td>\n",
              "      <td>0.154067</td>\n",
              "      <td>0.716667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17876</th>\n",
              "      <td>30962</td>\n",
              "      <td>life string disappointment failure .something ...</td>\n",
              "      <td>-0.069697</td>\n",
              "      <td>0.527273</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36204</th>\n",
              "      <td>62986</td>\n",
              "      <td>something hear time regard suicide .parent bes...</td>\n",
              "      <td>-0.102679</td>\n",
              "      <td>0.611607</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2296</th>\n",
              "      <td>4015</td>\n",
              "      <td>afraid happen cant control.</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20274</th>\n",
              "      <td>35034</td>\n",
              "      <td>allawardings allowlivecomments false author de...</td>\n",
              "      <td>-0.333333</td>\n",
              "      <td>0.659877</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  ... labels\n",
              "20179  34855  ...      0\n",
              "32455  56355  ...      0\n",
              "4445    7780  ...      0\n",
              "9736   16947  ...      0\n",
              "21736  37566  ...      1\n",
              "16908  29308  ...      0\n",
              "17876  30962  ...      1\n",
              "36204  62986  ...      1\n",
              "2296    4015  ...      1\n",
              "20274  35034  ...      1\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6uJuVM4Fper"
      },
      "source": [
        "def preprocess_text(sentence):\n",
        "    sentence = sentence.lower() # Convert to lowercase\n",
        "    sentence = re.sub(r'[^\\x00-\\x7F]+',' ', sentence) # Remove words with non-ASCII characters\n",
        "    words = sentence.split()\n",
        "    words = filter(lambda x: x[0]!= '@' , sentence.split()) # Remove user tags\n",
        "    words = [word for word in words if word not in set(stopwords.words('english'))] # Remove stop words\n",
        "    sentence = \" \".join(words)\n",
        "    return sentence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "IT8ouWPmFpgu",
        "outputId": "f3c70cc1-c039-405c-e881-4c7ff4ad24b8"
      },
      "source": [
        "data['preprocessedsentence'] = data.sentence.apply(preprocess_text)\n",
        "display(data.head())\n",
        "data1 = data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "      <th>labels</th>\n",
              "      <th>preprocessedsentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>always waited thing get better .always waited ...</td>\n",
              "      <td>0.057035</td>\n",
              "      <td>0.612771</td>\n",
              "      <td>0</td>\n",
              "      <td>always waited thing get better .always waited ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>tonight night dont care fucking hurt . . . ..</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1</td>\n",
              "      <td>tonight night dont care fucking hurt . . . ..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>im tucking tired people pretend care say theyr...</td>\n",
              "      <td>0.123889</td>\n",
              "      <td>0.582778</td>\n",
              "      <td>0</td>\n",
              "      <td>im tucking tired people pretend care say theyr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>sucidal intention turned cutting stop many cut...</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.558333</td>\n",
              "      <td>0</td>\n",
              "      <td>sucidal intention turned cutting stop many cut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>slowly becoming mentally unstable show.</td>\n",
              "      <td>0.337500</td>\n",
              "      <td>0.512500</td>\n",
              "      <td>0</td>\n",
              "      <td>slowly becoming mentally unstable show.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                               preprocessedsentence\n",
              "0   1  ...  always waited thing get better .always waited ...\n",
              "1   6  ...      tonight night dont care fucking hurt . . . ..\n",
              "2   7  ...  im tucking tired people pretend care say theyr...\n",
              "3   9  ...  sucidal intention turned cutting stop many cut...\n",
              "4  10  ...            slowly becoming mentally unstable show.\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUzxNTVkFpjn",
        "outputId": "5a9eeb4d-b7cb-4e6b-e200-c035c49ffa78"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksL43gNEShQ"
      },
      "source": [
        "example1 = \"/content/drive/MyDrive/glove.6B.50d.txt\"\n",
        "file1 = open(example1, \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiRtvTXrESja",
        "outputId": "5d7357e1-acbd-41f7-add3-ca30ca0fe75c"
      },
      "source": [
        "file1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/drive/MyDrive/glove.6B.50d.txt' mode='r' encoding='UTF-8'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3TgHZD5Ga_q"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = file1\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHmBGjR6GbB3",
        "outputId": "ce86f21c-ecf4-4e11-a69c-a95fa9bf5ff2"
      },
      "source": [
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.TextIOWrapper name='/content/drive/MyDrive/glove.6B.50d.txt' mode='r' encoding='UTF-8'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGNsU3oTGbEB",
        "outputId": "a14b8193-0788-4616-908a-953c1694e3fa"
      },
      "source": [
        "data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                      0\n",
              "sentence                0\n",
              "polarity                0\n",
              "subjectivity            0\n",
              "labels                  0\n",
              "preprocessedsentence    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeQx9VTlGbGZ"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_length = data.preprocessedsentence.apply(lambda x: len(x.split())).max()\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(data.preprocessedsentence)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "encoded_sentences = t.texts_to_sequences(data.preprocessedsentence)\n",
        "padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
        "\n",
        "vocab_size = len(t.word_index) + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ1E2FE1GbID"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIA2Ty3LGbK2"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 50))\n",
        "for word, i in t.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le6639tGESly"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(padded_sentences, data.labels, test_size=0.1, stratify=data.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw6W3UYKESoQ"
      },
      "source": [
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(vocab_size, 50, input_length=max_length, weights=[embedding_matrix], trainable=True))\n",
        "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Bidirectional(LSTM(20)))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(BatchNormalization())\n",
        "model_glove.add(Dense(64, activation='relu'))\n",
        "model_glove.add(Dense(64, activation='relu'))\n",
        "model_glove.add(Dense(1, activation='sigmoid'))\n",
        "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKfbDg0dESqf",
        "outputId": "30ea3497-c44a-410a-af61-5e156479dcfe"
      },
      "source": [
        "model_glove.fit(x_train, y_train, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1786/1786 [==============================] - 1247s 691ms/step - loss: 0.6293 - accuracy: 0.6260\n",
            "Epoch 2/5\n",
            "1786/1786 [==============================] - 1258s 704ms/step - loss: 0.3655 - accuracy: 0.8407\n",
            "Epoch 3/5\n",
            "1786/1786 [==============================] - 1270s 711ms/step - loss: 0.2594 - accuracy: 0.8977\n",
            "Epoch 4/5\n",
            "1786/1786 [==============================] - 1276s 715ms/step - loss: 0.2114 - accuracy: 0.9200\n",
            "Epoch 5/5\n",
            "1786/1786 [==============================] - 1278s 715ms/step - loss: 0.1728 - accuracy: 0.9376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac51ad2198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_is0cQ90Xigw",
        "outputId": "bd9ba701-2012-4da3-c746-d3906f4e6df0"
      },
      "source": [
        "model_glove.fit(x_test, y_test, epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "199/199 [==============================] - 143s 720ms/step - loss: 0.2946 - accuracy: 0.8814\n",
            "Epoch 2/5\n",
            "199/199 [==============================] - 144s 722ms/step - loss: 0.2226 - accuracy: 0.9123\n",
            "Epoch 3/5\n",
            "199/199 [==============================] - 144s 723ms/step - loss: 0.1828 - accuracy: 0.9368\n",
            "Epoch 4/5\n",
            "199/199 [==============================] - 144s 723ms/step - loss: 0.1385 - accuracy: 0.9532\n",
            "Epoch 5/5\n",
            "199/199 [==============================] - 143s 720ms/step - loss: 0.1229 - accuracy: 0.9606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac4e33f208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BebaYWrbhLNs"
      },
      "source": [
        "batch_size=64\n",
        "\n",
        "gen = ImageDataGenerator(horizontal_flip = True,\n",
        "                         vertical_flip = True,\n",
        "                         width_shift_range = 0.1,\n",
        "                         height_shift_range = 0.1,\n",
        "                         zoom_range = 0.1,\n",
        "                         rotation_range = 10\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}